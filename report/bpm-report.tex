\documentclass[a4paper,10pt]{article}
\usepackage{geometry}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}
\usepackage{syntax}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{subfig}
\usepackage[strict]{changepage}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\begin{document}

\title{\vspace{-1.7cm}Inducing Probabilistic Programs by Bayesian Program Merging}
\author{Irvin Hwang\\Stanford University  \and Andreas Stuhlm\"{u}ller\\MIT \and Noah D. Goodman\\Stanford University}
\date{}
\maketitle
\thispagestyle{empty}
\begin{abstract}

This report outlines an approach to learning generative models from data. We express models as probabilistic programs, which allows us to capture abstract patterns within the examples without sacrificing the ability to accurately represent given examples. By choosing our language for programs to be an extension of the algebraic data type of the examples, we can begin with a program that generates all and only the examples. We then introduce greater abstraction, and hence generalization, incrementally to the extent that it is explanatory. Motivated by previous approaches to model merging and program induction, we search for such explanatory abstractions using program transformations. We consider two types of transformation: {\em Abstraction} merges common subexpressions within a program into new functions (anti-unification). {\em Deargumentation} simplifies functions by reducing the number of arguments. We demonstrate that this approach finds key patterns in the domain of nested lists, including parametrized sub-functions and stochastic recursion.

\end{abstract}

\tableofcontents

\section{Introduction}

What patterns do you see when you look at figure \ref{fig:plants}? You might describe the image as a series of trees, where each tree has a large, blueish base and a number of green branches of variable length, with each branch ending in a flower that is either red or purple. Recognizing such patterns is an important aspect of intelligence. Understanding algorithms that can identify patterns contributes to our understanding of human and machine intelligence.
One way to approach this pattern recognition problem is as the problem of learning generative models for observed examples: we wish to find a description of the process that gave rise to a set of examples, and we form this description in a rich language for generative processes---a probabilistic programming language. 
In this document, we build on the representation in \cite{A.Stuhlmueller:2010:6d11a} and explore a family of algorithms for learning probabilistic programs.

\begin{figure}[b]
\begin{center}
\includegraphics[scale=.60]{../figures/trees.pdf}
\end{center}
\caption{Tree-like objects.}
\label{fig:plants}
\end{figure}

%A difficulty with the identification of regularities in data is that there are many notions of what it means to be a pattern and how this can vary with the types of data being analyzed.  We will approach this problem by first transforming data (whatever its type) into a canonical form (an expression in a programming language) and then defining pattern as repeated computation in this program.  

%some high-level discussion here, about what kinds of patterns are important to find, and needing a language that spans from very concrete to fully abstract?

The main components of our approach are as follows: We represent data in terms of algebraic data types, we represent patterns in data as probabilistic programs, we guide search through program space using the Bayesian posterior probability, and we create search moves by detecting repeated computation. Our algorithm 
(1) turns data into a large program that generates all and only the examples,
(2) identifies (approximately) repeated structure in the large program and transforming the program to make sharing explicit, and   %does this describe dearg well?
(3) continues to propose program transformations, accepting only those that increase the posterior probability of the program given the data.  
The probabilistic programs learned in this manner can be understood as generative models and reasoning about such models can be formulated in terms of probabilistic inference. We illustrate these ideas on list-structured data.

Generative models play a prominent role in modern machine learning and in understanding different classes of models, e.g., Hidden Markov models and probabilistic context-free grammars, and have led to a wide variety of applications.  There is a trade-off between the variety of patterns a model class is able to capture and the feasibility of learning models in that class \cite{Russell2003}.  Much of machine learning has focused on studying classes of models with limited expressiveness in order to develop tractable algorithms for modeling large data sets.  Our investigation takes a different approach and explores how learning might proceed in an expressive class of models with a focus on identifying abstract patterns from small amounts of data.

We represent generative models as programs in a probabilistic programming language. A probabilistic program represents a probability distribution, and each evaluation of the program results in a sample from the distribution. We implement these programs in a subset of the probabilistic programming language Church \cite{N.D.Goodman:2008:f2a0d}.  
These programs can have parameterized functions and recursion, which allow for natural representation of ``long-range'' dependencies and recursive patterns.  We will frame searching this space of models in terms of Bayesian model merging \cite{Stolcke:1994:IPG:645515.658235} and we demonstrate that this approach can find interesting patterns in the list data domain.

Before we proceed, a note about what this report is and is not: This report \emph{is} a status update on our working system, containing detailed code and illustrative examples. It documents some progress we have made that we believe can be useful more generally.
This report \emph{is not} a completed academic work. In particular, it does little to situate our work within the context of previous work (some of which has  directly inspired us), provides little high-level discussion, and aims for illustrative examples rather than compelling applications.

\subsection{Bayesian model merging}

Bayesian model merging is a framework for searching through a space of generative models in order to find a model that accurately generates the observed data.  The main idea is to search model space through a series of merge transformations and to use the posterior $P(M|D)$ of model $M$ given data $D$ as a criterion for selecting transformations.

We create an initial model by building a program that has a uniform distribution over the training set ({\em data incorporation}).  While this initial model has high likelihood $P(D|M)$, it never generates points outside the training set---it severely overfits the initial data. Therefore, we generate alternative model hypotheses using program transformations that collapse model structure and that often result in better generalizations.

This technique has been successfully applied to learning artificial probabilistic grammars represented as hidden Markov models, n-grams, and probabilistic context-free grammars \cite{Stolcke:1994:IPG:645515.658235}.


\subsection{Bayesian program merging}

We extend Bayesian model merging to models expressed in the rich class of probabilistic programs.  This allows us to represent complex patterns and to use a wide range of transformations, including transformations that correspond to lossy data compression. In the remainder of this document, we will describe the following parts required to implement Bayesian program merging:

\begin{description}
\item[Data representation and program language]  \hfill \\
  We represent data in terms of an algebraic data type, which gives us a way to form initial programs using the type constructors. Our language for programs consists of type constructors and additional operators. This language is probabilistic, hence programs correspond to distributions on observations. Program structure corresponds to regularities in the observations.
\item[Search objective: Posterior probability of probabilistic programs]  \hfill \\
  The objective for our search through program space is the posterior probability of a program given the observed data. This posterior decomposes into a prior based on program length and into a likelihood that we estimate using selective model averaging.
\item[Search moves: Program transformations]  \hfill \\
  We describe two program transformations that we use as search moves, abstraction and deargumentation. Both transformations collapse program structure, which often increases prior probability of the program and improves generalization to unobserved data.
  \item[Search strategy: Beam search]  \hfill \\
  Given a space of programs, an objective function, and search moves between programs (transformations), we still need to specify a search strategy. We use beam search.
\end{description}

\newpage
\section{Data representation and program language}
We assume that we can represent our data using an algebraic data type. This assumption gives us a starting point for program induction, since any data can be directly translated into a program which is a derivation (sequence of data constructor operations) of the data from the type specification. 
\subsection{Data and program representation in the list domain}
We can model the trees shown in figure \ref{fig:plants} in terms of nested lists (s-expressions).  Each tree consists of nodes, where each node has a size and color along with a list of child nodes.
\begin{grammar}
<tree> ::= nil

<tree> ::= (node <data> <tree> <tree> ... )

<data> ::= (data <color> <size>)

<color> ::= (color [number])

<size> ::= (size [number])
\end{grammar}
The following is an example of a tree expression along with a graphical representation.
\lstset{basicstyle=\small\ttfamily, frameround=fttt, breaklines=true, language=Lisp, showstringspaces=false, breakatwhitespace=true}
\begin{lstlisting}[mathescape=true]
(node (data (color 50) (size 1))
  (node (data (color 30) (size 0.3))
    (node (data (color 225) (size 0.3)))
    (node (data (color 225) (size 0.3)))
    (node (data (color 225) (size 0.3)))))
 $\includegraphics[scale=1.0]{../figures/tree.pdf}$ 
\end{lstlisting}

We now have a way of representing data as rudimentary programs.  In order to capture interesting patterns, we need a more expressive language.  We use the following subset of Church for Bayesian program merging in the tree domain:
\begin{grammar}
<expr> ::= (begin <expr> <expr> ... ) 
\alt (lambda (<var> ...) <expr>)
\alt (<expr> <expr>)  
\alt (define <var> <expr>)
\alt (if (flip [number]) <expr> <expr>)
\alt <var> 
\alt <primitive>

<var> ::= V[number] | F[number] 

<primitive> ::= uniform-choice | list | node | data | color | size | [number]
\end{grammar}

\subsection{Data incorporation}
The first step of Bayesian program merging is data incorporation.  Data incorporation is the creation of an initial model by going through each example in the training set and creating an expression that evaluates to this example (in terms of the algebraic data type constructors).  We combine these programs into a single expression that draws uniformly from this list.
% We assume Gaussian noise in the color attribute for simplicity's sake, but discuss a possible direction for learning such a property later in the report.
\begin{lstlisting}[frame=trblsingle]
(define (incorporate-data trees)
  `(lambda () (uniform-choice ,@(map tree->expression trees))))

(define (tree->expression tree)
  (if (null? tree)
      '()
      (pair 'node
            (pair (node-data->expression (first tree)) 
                  (map tree->expression (rest tree))))))

(define (node-data->expression lst)
  `(data (color (gaussian ,(first (second lst)) 25)) 
	 (size ,(first (third lst)))))
\end{lstlisting}

\begin{figure}[t]
  \subfloat{\lstinputlisting[boxpos=b]{treedata.ss}}
  \hspace{40pt} %
  \subfloat{
    \includegraphics[scale=1.2]{../figures/initData.pdf}
  }
\caption{A small set of tree observations.}
\label{fig:tree-obs}
\end{figure}

In our implementation, the data are unannotated s-expressions (e.g., \texttt{((1) (2))}), which can easily be converted into expressions in terms of the tree type constructors (the tree expression for the above example is \texttt{(node (data (color 1) (size 2)))}).  In this report, we assume for readability that all data is in tree expression form. An important question we leave for future work is how to perform data incorporation when the data is less structured and, for example, given in terms of feature vectors. Calling \texttt{incorporate-data} on the program shown in figure \ref{fig:tree-obs} results in the program 
\begin{lstlisting}[mathescape=true]
(lambda ()
  (uniform-choice
   (node (data (color (gaussian 70 25)) (size 1))
         (node (data (color (gaussian 37 25)) (size 0.3))
               (node (data (color (gaussian 213 25)) (size 0.3)))
               (node (data (color (gaussian 207 25)) (size 0.3)))
               (node (data (color (gaussian 211 25)) (size 0.3)))))
   (node (data (color (gaussian  43)) (size 1))
         (node (data (color (gaussian 47 25)) (size 0.1))
               (node (data (color (gaussian 33 25)) (size 0.3))
                     (node (data (color (gaussian 220 25)) (size 0.3)))
                     (node (data (color (gaussian 224 25)) (size 0.3)))
                     (node (data (color (gaussian 207 25)) (size 0.3))))))))
$\includegraphics[scale=.7]{../figures/initProgram.pdf}$
\end{lstlisting}


\newpage
\section{Search objective: Posterior probability of probabilistic programs}
A generative model is a joint probability distribution over latent states and observed data. We can represent such a joint distribution as a program in a probabilistic programming language like Church \cite{N.D.Goodman:2008:f2a0d}. The structure of the program, i.e., the decomposition into functions and the flow of control, can capture regularities in the data. We illustrate this idea using a probabilistic program that generates the images in figure \ref{fig:plants}. Different parts of the program correspond to the different patterns we described earlier (to improve readability, we use semantically meaningful function and variable names instead of \texttt{F[number]} and \texttt{V[number]} as specified in our grammar).  
\begin{lstlisting}[mathescape=true]
(define (tree)
  (uniform-choice
   (node (body) (branch))
   (node (body) (branch) (branch))
   (node (body) (branch) (branch) (branch))
   (node (body) (branch) (branch) (branch) (branch))))

(define (body) 
  (data (color (gaussian 50 25)) (size 1)))

(define (branch)
  (if (flip .2) 
      (flower (if (flip .5) 150 255))
      (node (branch-info) (branch))))

(define (branch-info)
  (data (color (gaussian 0 25)) (size .1)))

(define (flower shade)
  (node (data (color (gaussian 0 25)) (size .3))
        (petal shade)
        (petal shade)
        (petal shade)))

(define (petal shade)
  (node (data (color shade) (size .3))))
$\includegraphics[scale=.35]{../figures/trees2.pdf}$
\end{lstlisting}
Our programs are a combination of data constructors, control flow operations, and lambda abstractions.  When we call \texttt{(tree)}, the program shown above first determines the number of branches the tree will have and then creates a large body node that connects these branches.  Each branch function recursively connects a series of small nodes together and ends in a call to the flower function, passing it one of two colors.  The flower function creates a node with three identically colored ``petal'' nodes.  The program structure reflects patterns such as a flower having three petals and the body having a large base.  The compositionality of the language captures relational patterns such as the fact that branches end in flowers. Given that programs can represent such structures, our goal is to transform the initial program generated by data incorporation into such a more structured form.

\subsection{Abstract syntax utilities}
Some program transformations generate new functions. We call such functions {\em abstractions}. We represent them using a name, argument variables, and a pattern, i.e., the s-expression that makes up the body of the function. In the following code snippet, we use the \texttt{sym} function that creates readable unique symbols for function and variable names.
\begin{lstlisting}[frame=trbl]
(define (make-abstraction body variables)
  (make-named-abstraction (sym FUNC-SYMBOL) body variables))
(define (make-named-abstraction name body variables)
  (list 'abstraction name variables body))

(define abstraction->name second)
(define abstraction->vars third)
(define abstraction->body fourth)
\end{lstlisting}

{\em Programs} represent the generative models we search over. They consist of a list of abstractions and a body.
\begin{lstlisting}[frame=trbl]
(define (make-program abstractions body)
  (list 'program abstractions body))
(define program->abstractions second)
(define program->body third)
\end{lstlisting}  

We wrap programs into another data type to keep track of additional information during search. We motivate this in the section on search in more detail. The basic idea is to avoid recomputation of a program's likelihood (an expensive computation) when transformations do not affect a program's semantics.
\begin{lstlisting}[frame=trbl]
(define (make-program+ program posterior log-likelihood log-prior semantics-preserved)
  (list 'program+ program posterior log-likelihood log-prior 
        semantics-preserved))
(define program+->program second)
(define program+->posterior third)
(define program+->log-likelihood fourth)
(define program+->log-prior fifth)
(define program+->semantics-preserved sixth)
(define (program+->program-transform semantics-preserved program+ new-program)
  (make-program+ new-program 
                 (program+->posterior program+) 
                 (program+->log-likelihood program+) 
                 (program+->log-prior program+) 
                 semantics-preserved))
\end{lstlisting}


\subsection{Posterior probability}

Probabilistic programs correspond to probability distributions on observed data, i.e., we can compute the likelihood of a given observation under a program. Given a process for generating programs (a prior), we use Bayes theorem to compute the posterior probability of a program:
\begin{equation}P(M|D)\propto P(D|M)P(M)\end{equation}
Here, $P(D|M)$ is the probability that program $M$ generates data $D$, the likelihood, and $P(M)$ is the prior probability of program $M$. We use a prior based on program length:
\begin{equation}P(M)\propto e^{-\alpha \mathit{size}(M)}\end{equation}
\begin{lstlisting}[frame=trbl]
(define (log-prior program)
  (- (* alpha (program-size program))))
\end{lstlisting}
This prior biases the search towards smaller programs. Increasing the constant $\alpha$ gives the prior more weight when calculating the posterior, which means that minimizing program size is a more important criterion. A program's size is the number of symbols in the function bodies as well as in the main body.
\begin{lstlisting}[frame=trbl]
(define (sexpr-size sexpr)
  (if (list? sexpr)
      (apply + (map sexpr-size sexpr))
      1))

(define (abstraction-size abstraction)
  (sexpr-size (abstraction->body abstraction)))
  
(define (program-size program)
  (let* ([abstraction-sizes (apply + (map abstraction-size (program->abstractions program)))]
         [body-size (sexpr-size (program->body program))])
    (+ abstraction-sizes body-size)))
\end{lstlisting}
Computing the likelihood is the difficult part of the posterior probability computation. Intuitively, we can think of the likelihood as tracking how good a particular program is at producing a set of target data.  This is important in search, since it gives precise, quantitative information on whether and how to adjust our hypothesis program. However, since there may be a large number of possible settings for the random choices of a program, this can make determining which choices lead to the observed data difficult. Furthermore, for any given data point, there could be multiple settings that generate this data point; to correctly compute the likelihood, we need to take into account all of them. Often, we cannot compute this quantity exactly. In the following, we describee a way to approximate this computation for list data.

\subsection{Likelihood estimation in the list domain}

In the case of programs that generate list-structured data, we can estimate the likelihood by (1) generating samples using a sequential Monte Carlo method (\texttt{smc}) that splits the data generation process into generation of the discrete list topology and the continuous value topology and (2) applying selective model averaging to these samples. We begin by factoring the problem of generating the data set into the problem of generating each of the data points:
\begin{eqnarray}
P(T|M) &=& \prod_{t \in M}P(t|M)
\end{eqnarray}
Here, $T$ is the observed set of trees, $t$ a single tree, and $M$ the generative model (program).
\begin{lstlisting}[frame=trbl]
(define (log-likelihood trees prog sample-size)
  (apply + (map (lambda (tree) (single-log-likelihood prog sample-size tree)) trees)))
\end{lstlisting}
We will estimate the likelihood of a single tree, $P(t|M)$, by evaluating the program many times, restricting the computation to result in the target tree each time. The product of the probabilities of all random choices made during a single evaluation corresponds to the probability of a single possible way of generating the tree. Since there may be multiple ways for a program to generate a given tree, we sum up the probability of each {\em distinct} parse to compute a lower bound on the true likelihood (selective model averaging).
\begin{lstlisting}[frame=trbl]
(define (single-log-likelihood program popsize tree)
  (let* ([program* (replace-gaussian (desugar program))]
         [model (eval (program->sexpr program*))]
         [topology-scores+tree-parameters (compute-topology-scores+evaluate model tree popsize)]
         [topology-scores (first topology-scores+tree-parameters)]
         [trees-with-parameters (second topology-scores+tree-parameters)]
         [data-scores (map (lambda (tree-with-parameters) (compute-data-score tree tree-with-parameters)) trees-with-parameters)] ;;remove -inf?
         [scores (delete -inf.0 (map + topology-scores data-scores))]
         [score (if (null? scores)
                     -inf.0
                     (apply log-sum-exp scores))])
    score))
\end{lstlisting}
We take advantage of the fact we can directly compute the probability of a sample from a Gaussian given the parameters.  We therefore modify the Gaussian functions in the program to output the mean and variance for a particular node instead of sampling a value from the distribution.  The code below also changes the \texttt{uniform-choice} syntactic construct into a \texttt{uniform-draw}, which current Church implementations provide.
\begin{lstlisting}[frame=trbl]
(define (replace-gaussian program)
  (define (gaussian? sexpr)
    (tagged-list? sexpr 'gaussian))
  (define (return-parameters sexpr)
    `(list 'gaussian-parameters ,(second sexpr) ,(third sexpr)))
  (define (replace-in-abstraction abstraction)
    (make-named-abstraction (abstraction->name abstraction) (sexp-search gaussian? return-parameters (abstraction->pattern abstraction)) (abstraction->vars abstraction)))
  (let* ([converted-abstractions (map replace-in-abstraction (program->abstractions program))]
	 [converted-body (sexp-search gaussian? return-parameters (program->body program))])
    (make-program converted-abstractions converted-body)))

(define (desugar program)
  (define (uniform-choice? sexpr)
    (tagged-list? sexpr 'uniform-choice))
  (define (uniform-draw-conversion sexpr)
    `((uniform-draw (list ,@(map thunkify (rest sexpr))))))
  (define tests+replacements (zip (list uniform-choice?) (list uniform-draw-conversion)))
  (define (apply-transforms sexpr)
    (fold (lambda (test+replacement expr)
	    (sexp-search (first test+replacement) (second test+replacement) expr))
	  sexpr
	  tests+replacements))
  (define (desugar-abstraction abstraction)
    (make-named-abstraction (abstraction->name abstraction) (apply-transforms (abstraction->pattern abstraction)) (abstraction->vars abstraction)))
  (let* ([converted-abstractions (map  desugar-abstraction (program->abstractions program))]
	 [converted-body (apply-transforms (program->body program))])
    (make-program converted-abstractions converted-body)))

(define (thunkify sexpr) `(lambda () ,sexpr))
\end{lstlisting}
In the code below, \texttt{smc-core} forces the program to generate the desired data.  A detailed description of \texttt{smc-core} is beyond the scope of this report. In short, the method is an incremental forward sampler; for this reason, we separate out the continuous choices, since forward sampling has probability $0$ of generating the observed values.
\begin{lstlisting}[frame=trbl]
(define (compute-topology-scores+evaluate model tree popsize)
  (let* ([smc-core-arguments (create-smc-core-args model tree popsize)]
         [samples (apply smc-core smc-core-arguments)]
         [repeat-symbol (find-repeat-symbol samples)]
         [unique-samples
          (fold (lambda (s a)
                  (if (member (mcmc-state->addrval s repeat-symbol)
                              (map (lambda (x) (mcmc-state->addrval x repeat-symbol)) a))
                      a (pair s a))) '() samples)]
         [topology-scores (map mcmc-state->score unique-samples)]
         [generated-trees (map mcmc-state->query-value unique-samples)])
    (list topology-scores generated-trees)))
\end{lstlisting}
We use the Gaussian parameters for each node's color to determine the probability of the observed color values of a tree:
\begin{lstlisting}[frame=trbl]
(define (compute-data-score tree tree-with-parameters)
    (if (null? tree)
        0
        (+ (single-data-score (node->data tree) (node->data tree-with-parameters)) (apply + (map compute-data-score (node->children tree) (node->children tree-with-parameters))))))

(define (single-data-score original-data parameterized-data)
  (let* ([color-score (score-attribute (data->color original-data) (data->color parameterized-data))]
         [size-score (score-attribute (data->size original-data) (data->size parameterized-data))])
    (+ color-score size-score)))

(define (score-attribute original-attribute parameterized-attribute)
  (if (tagged-list? parameterized-attribute 'gaussian-parameters)
      (log (normal-pdf (first original-attribute) (gaussian->mean parameterized-attribute) (gaussian->variance parameterized-attribute)))
      (if (= (first original-attribute) (first parameterized-attribute)) 0 -inf.0)))

(define gaussian->mean second)
(define gaussian->variance third)
\end{lstlisting}


\newpage
\section{Search moves: Program transformations}

Given a program that we generated using data incorporation, we want to propose changes to the program such that its posterior probability improves. We will achieve this mainly using program transformations that compress patterns in the program, thereby increasing its prior probability. In the following, we describe two such transformations, abstraction and deargumentation.

\subsection{Abstraction}

By abstraction, we denote the idea of creating new functions based on syntactic patterns in a program and replacing these patterns with calls to the newly created functions.
This removes duplication in the code and acts as a proxy for recognizing repeated computation.
In terms of Bayesian model merging, this transformation merges the structure of the model, which potentially leads to models with better generalization properties \cite{Stolcke:1994:IPG:645515.658235}. We can also interpret this process as finding partial symmetries as in the work of Bokeloh et al. \cite{DBLP:journals/tog/BokelohWS10}, which was successful in the domain of inverse-procedural modeling.

The following code fragment implements this procedure. The function \texttt{compressions} finds all (lambda) abstractions that can be formed by anti-unifying (partially matching) pairs of subexpressions in a condensed form of the program (only the bodies of the functions and the body of the program). We filter out duplicate abstractions and then create compressed programs by replacing occurrences of the abstraction bodies in the original program.

\begin{lstlisting}[frame=trbl]
(define (compressions program . nofilter)
  (let* ([condensed-program (condense-program program)]
         [abstractions (possible-abstractions condensed-program)]
         [compressed-programs (map (curry compress-program program) abstractions)]
         [prog-size (program-size program)]
         [valid-compressed-programs
          (if (not (null? nofilter))
              compressed-programs
              (filter (lambda (cp)
                        (<= (program-size cp)
                            (+ prog-size 1)))
                      compressed-programs))])
    valid-compressed-programs))

(define (condense-program program)
  `(,@(map abstraction->body (program->abstractions program))
    ,(program->body program)))

(define (possible-abstractions expr)
  (let* ([subexpr-pairs (list-unique-commutative-pairs (all-subexprs expr))]
         [abstractions (map-apply (curry anti-unify-abstraction expr) subexpr-pairs)])
    (filter-abstractions  abstractions)))
\end{lstlisting}

The following example illustrates the abstraction transformation on a language that slightly differs from the tree example.
\begin{lstlisting}[mathescape=true]
(uniform-choice 
 (node
  (node a (node a (node b) (node b)))
  (node a (node a (node c) (node c)))))
$\includegraphics[scale=.40]{../figures/csrExample.pdf}$
\end{lstlisting}
A transformed version of this program looks like this:
\begin{lstlisting}[mathescape=true]
(begin
  (define (F1 V1 V2)
    (node a (node a (node V1) (node V2))))
  (uniform-choice (node (F1 b b) (F1 c c))))
$\includegraphics[scale=.40]{../figures/csrDecompose1.pdf}\\
\includegraphics[scale=.40]{../figures/csrDecompose2.pdf}$
\end{lstlisting}
Both programs have the same behavior, i.e., this transformation preserves semantics. Both programs return \texttt{(a (a (b) (b)))} and \texttt{(a (a (c) (c)))} with equal probability.

This transformation refactors a program with subexpressions that partially match into a program with a function which contains the common parts of the matching subexpressions as its body.  In the example shown above, the subexpressions that partially match are \texttt{(node a (node a (node b) (node b)))} and \texttt{(node a (node a (node c) (node c)))}.  The common subexpression is \texttt{(node a (node a (node x) (node y)))}. The function created using this common subexpression is \texttt{F1} and the original subexpressions are replaced with \texttt{(F1 b b)} and \texttt{(F1 c c)}. (There is further potential for compression here, which we will address using the deargumentation transformation.)

\subsubsection{Anti-unification}

There exists an abstraction transformation for each pair of subexpressions that have a partial match.  In the case of \texttt{(+ (+ 2 2) (- 2 5))} the following pairs of subexpressions have a partial match: \texttt{[2,2], [(+ 2 2), (- 2 5)], [(+ 2 2), (+ (+ 2 2) (- 2 5))]}. The process of finding a partial match between two expressions is called anti-unification.

One way to understand the process is in terms of the syntax trees for the expressions.  Every s-expression is a tree where the lists and sublists of the s-expression make up the interior nodes and the primitive elements of the lists (e.g., symbols, numbers) are the leaves.  The tree in figure \ref{expressionTree} corresponds to the expression \texttt{(+ (+ 2 2) (- 2 5))}. We can find a partial match between two expressions by finding a common subtree between their tree representations.
\begin{figure}[b]
\begin{center}
\includegraphics[scale=.40]{../figures/expressionTree.pdf}
\caption{An expression represented as a tree.}
\label{expressionTree}
\end{center}
\end{figure}

Anti-unification proceeds by recursively comparing two expressions, $A$ and $B$ (using the function \texttt{build-pattern}).  If $A$ and $B$ are the same primitive, this primitive is returned.  If $A$ and $B$ are lists $(a_1,\ldots,a_n)$ and $(b_1,\ldots,b_n)$ of the same length, a list $(c_1,\ldots,c_n)$ is returned, with each element $c_i$ being the anti-unification of $a_i$ and $b_i$.  Otherwise, a variable is returned.

\begin{lstlisting}[frame=trbl]
(define anti-unify
  (lambda (expr1 expr2)
    (begin
      (define variables '())

      (define (add-variable!)
        (set! variables (pair (sym (var-symbol)) variables))
        (first variables))
      
      (define (build-pattern expr1 expr2)
        (cond [(and (primitive? expr1) (primitive? expr2)) (if (equal? expr1 expr2) expr1 (add-variable!))]
              [(or (primitive? expr1) (primitive? expr2)) (add-variable!)]
              [(not (eqv? (length expr1) (length expr2))) (add-variable!)]
              [else
               (let ([unified-expr (map (lambda (subexpr1 subexpr2) (build-pattern subexpr1 subexpr2))
                                        expr1 expr2)])
                 unified-expr)]))
      (let ([pattern (build-pattern expr1 expr2)])
        (list pattern (reverse variables))))))
\end{lstlisting}

We now illustrate the process of anti-unification using the expressions \texttt{(+ (+ 2 2) (- 2 5))} and \texttt{(+ (- 2 3) 4)}:
\begin{enumerate}
\item We first compare the root of the trees and make sure that we have lists of the same size. In this case, they are both of size $3$, therefore we have matching roots. \\
  Partial match: \texttt{(* * *)}
\item We recursively attempt to match the three subexpressions: \texttt{+} against \texttt{+}, \texttt{(+ 2 2)} against \texttt{(- 2 3)}, and \texttt{(- 2 )} with \texttt{4}. Since \texttt{+} and \texttt{+} are identical primitives, they match. \\
  Partial match: \texttt{(+ * *)}
\item Comparing \texttt{(+ 2 2)} and \texttt{(- 2 3)}, we see that they are both lists of size 3 and therefore they match. \\
  Partial match: \texttt{(+ (* * *) *)}
\item Again, we recursively match subexpressions of \texttt{(+ 2 2)} and \texttt{(- 2 3)}, i.e., \texttt{+} against \texttt{-}, \texttt{2} against \texttt{2}, and \texttt{2} against \texttt{3}. Since \texttt{+} and \texttt{-} are primitives that do not match, we replace them with a variable. \\
  Partial match: \texttt{(+ (V1 * *) *)}
\item We compare \texttt{2} to \texttt{2} and \texttt{2} to \texttt{3}. \\
  Partial match: \texttt{(+ (V1 2 V2) *)}
\item We compare \texttt{(- 2 5)} and \texttt{4} and find that there is no match since \texttt{(- 2 5)} is a list and \texttt{4} is a primitive. \\
  Final match: \texttt{(+ (V1 2 V2) V3)}.
\end{enumerate}


\subsubsection{Refactoring programs}
Using the lambda abstractions we created from the partial matches between two subexpressions of some program $P$, we now attempt to refactor the program, possibly compressing it. For a given abstraction, we take the abstraction body and replace all subexpressions of $P$ that match this pattern by a function call.  We apply this replacement to all functions in $P$ as well as to the body of $P$ and insert the definition of our abstraction into the body of $P$ to generate a refactored program $P'$.

\begin{lstlisting}[frame=trbl]
(define (compress-program program abstraction)
  (let* ([compressed-abstractions (map (curry compress-abstraction abstraction) (program->abstractions program))]
         [compressed-body (replace-matches (program->body program) abstraction)])
    (make-program (pair abstraction compressed-abstractions)
                  compressed-body)))

(define (compress-abstraction compressor compressee)
  (make-named-abstraction (abstraction->name compressee)
                          (replace-matches (abstraction->body compressee) compressor)
                          (abstraction->vars compressee)))                           
\end{lstlisting}
We find and replace pattern matches of an abstraction $F$ in an expression $E$ by recursively matching the body of $F$ against $E$ using unification. If a match exists, then we return a function call to $F$. If there is no match, we return $E$ with matches replaced in each of its subexpressions. If $E$ is a non-matching primitive, we return $E$.

\begin{lstlisting}[frame=trbl]
(define (replace-matches s abstraction)
  (let ([unified-vars (unify s
                             (abstraction->body abstraction)
                             (abstraction->vars abstraction))])
    (if (false? unified-vars)
        (if (list? s)
            (map (lambda (si) (replace-matches si abstraction)) s)
            s)
        (pair (abstraction->name abstraction)
              (map (lambda (var) (replace-matches (rest (assq var unified-vars)) abstraction))
                   (abstraction->vars abstraction))))))
\end{lstlisting}
We illustrate refactoring using the expression \texttt{(+ (+ 2 2) (- 2 5))}. The partial match resulting from anti-unification between the subexpressions \texttt{[(+ (+ 2 2) (- 2 5)), (+ 2 2)]} is \texttt{(+ V1 V2)}.  We refactor the original expression \texttt{(+ (+ 2 2) (- 2 5))} in terms of \texttt{(+ V1 V2)} by creating a function \texttt{(define (F1 V1 V2) (+ V1 V2))} and replacing occurrences of its body in the original expression.  For example, one such replacement is \texttt{(F1 (+ 2 2) (- 2 5))}, another is \texttt{(+ (F1 2 2) (- 2 5))}.  In general, we apply the function wherever possible and get a refactored program such as:
\begin{lstlisting}
(+ (+ 2 2) (- 2 5))
=>
(begin
  (define (F1 V1 V2) (+ V1 V2))
  (F1 (F1 2 2) (- 2 5)))
\end{lstlisting}
The input to the refactoring procedure is a function $F$ created from anti-unification and an expression $E$ which will be refactored in terms of $F$.  In the example above, $F$ is \texttt{(define (F1 V1 V2) (+ V1 V2))}, $E$ is \texttt{(+ (+ 2 2) (- 2 5))}, and the result of refactoring is
\begin{lstlisting}
(begin
  (define (F1 V1 V2) (+ V1 V2))
  (F1 (F1 2 2) (- 2 5)))
\end{lstlisting}

In the example, \texttt{(+ (+ 2 2) (- 2 5))} matches \texttt{(+ V1 V2)}, therefore we return an application of \texttt{F1} to arguments \texttt{(+ 2 2)} and \texttt{(- 2 5)}, resulting in \texttt{(F1 (F1 2 2) (- 2 5))}\footnote{The function \texttt{F1} is equivalent to the addition function between two numbers, therefore refactoring the program in terms of this function does not compress the original expression at all.  An example where refactoring can compress an expression is the expression \texttt{[(+ (+ 2 2) (- 2 5)), (+ 2 2)]} with the abstraction \texttt{(define (F1 V1) (+ V1 V1))}. Here, the refactored expression is \texttt{(+ (F1 2) (- 2 5))}, which is shorter than \texttt{(+ (+ 2 2) (- 2 5))}.}.  


\subsubsection{Unification}

The problem of determining whether there is a match between an expression $E$ and an abstraction $F$ is known as unification \cite{Robinson:1965:MLB:321250.321253}. We have described anti-unification, the process of creating an abstraction given an expression. Unification is the opposite of this process.  The return value of successful unification is a list of assignments for the arguments of $F$ such that $F$ applied to these arguments results in $E$.

The unification algorithm recursively checks whether the body of $F$ and $E$ are lists of the same size.  If they are, then unification returns a list containing the unification of each of the subexpressions.  If they are not the same size or only one of them is a list, unification returns \texttt{false}.  If both expressions are primitives, unification returns true if they are equal, false otherwise.  In the case where the function expression of the unification is a variable, we return an assignment, i.e., the variable along with the expression passed to unification. If any subunifications have returned \texttt{false}, we return \texttt{false}. If a variable that repeatedly occurs in $F$ is assigned to different values in different places, we also return \texttt{false}.
Otherwise, unification succeeds and we return the assignment of each unique variable of $F$.

\begin{lstlisting}[frame=trbl]
(define unify
  (lambda (s sv vars)
    (begin
      (define (variable? obj)
        (member obj vars))

      ;;deals with a variable that occurs multiple times in sv
      (define (check/remove-repeated unified-vars)
        (let* ([repeated-vars (filter more-than-one (map (curry all-assoc unified-vars) (map first unified-vars)))])
          (if (and (all (map all-equal? repeated-vars)) (not (any false? unified-vars)))
              (delete-duplicates unified-vars)
              #f)))
      
      (cond [(variable? sv) (if (eq? s 'lambda) #f (list (pair sv s)))]
            [(and (primitive? s) (primitive? sv)) (if (eqv? s sv) '() #f)]
            [(or (primitive? s) (primitive? sv)) #f]
            [(not (eqv? (length s) (length sv))) #f]
            [else
             (let ([assignments (map (lambda (si sj) (unify si sj vars)) s sv)])
               (if (any false? assignments)
                   #f
                   (check/remove-repeated (apply append assignments))))]))))
\end{lstlisting}

We illustrate unification using the abstraction \texttt{(define (F1 V1 V2) (+ V1 V2))} and the expression \texttt{(+ (+ 2 2) (- 2 5))}.

\begin{enumerate}
  \item Since \texttt{(+ (+ 2 2) (- 2 5))} and \texttt{(+ V1 V2)} are of the same length, we apply unification to the subexpression pairs \texttt{[+,+], [(+ 2 2), V1], [(- 2 5), V2]}.
  \item Unification between \texttt{+} and \texttt{+} returns the empty assignment list since neither is a variable and they match.
  \item Unification between \texttt{(+ 2 2)} and \texttt{V1} returns the assignment of \texttt{(+ 2 2)} to \texttt{V1} and likewise for \texttt{(- 2 5)} and \texttt{V2}.
  \item It follows that the function \texttt{F1} matches the expression \texttt{(+ (+ 2 2) (- 2 5))} with variable assignments \texttt{V1:=(+ 2 2)} and \texttt{V2:=(- 2 5)}.
\end{enumerate}

If the expression had been \texttt{(- (+ 2 2) (- 2 5))}, then unification between the outer \texttt{-} of the expression and the \texttt{+} of \texttt{F1} would have returned \texttt{false} and unification would have failed.

\subsubsection{Summary}

Abstraction is a program transformation that identifies repeated computation in a program by finding syntactic patterns. If the program has been generated using data incorporation, syntactic patterns directly correspond to patterns in the observed data. While this formalization of the notion of a pattern may seem limited at first, it is worth contemplating the central role of lambda abstraction in the lambda calculus and the expressiveness of this language.

The abstraction process has two steps: First, create abstractions from common subexpressions in a program using anti-unification. Second, compress the program using these abstractions by replacing instances of the abstractions with function calls via unification.
We now illustrate this process using the tree example that we first used in the section on data incorporation (figure \ref{fig:tree-obs}).
For this program, anti-unification finds $17$ possible abstractions. As examples, we show both the abstraction that results in the best compression (smallest program) and the abstraction that results in the 5th smallest program:
\begin{lstlisting}
(abstraction F1 (V1 V2)
             (data (color (gaussian V1 25)) (size V2)))

(abstraction F1 (V1 V2 V3 V4)
             (node (data (color (gaussian V1 25)) (size 0.3))
                   (node (data (color (gaussian V2 25)) (size 0.3)))
                   (node (data (color (gaussian V3 25)) (size 0.3)))
                   (node (data (color (gaussian V4 25)) (size 0.3)))))
\end{lstlisting}
The programs compressed using these abstractions look like this (size $55$ and $66$):
\begin{lstlisting}
(program
 ((abstraction F1 (V1 V2)
               (data (color (gaussian V1 25)) (size V2))))
 (uniform-choice
  (node (F1 70 1)
        (node (F1 37 0.3) (node (F1 213 0.3))
              (node (F1 207 0.3)) (node (F1 211 0.3)))))
 (node (F1 43 1)
       (node (F1 47 0.1)
             (node (F1 33 0.3) (node (F1 220 0.3))
                   (node (F1 224 0.3)) (node (F1 207 0.3))))))

(program
 ((abstraction F1 (V1 V2 V3 V4)
               (node (data (color (gaussian V1 25)) (size 0.3))
                     (node (data (color (gaussian V2 25)) (size 0.3)))
                     (node (data (color (gaussian V3 25)) (size 0.3)))
                     (node (data (color (gaussian V4 25)) (size 0.3))))))
 (uniform-choice
  (node (data (color (gaussian 70 25)) (size 1))
        (F1 37 213 207 211)))
 (node (data (color (gaussian 43 25)) (size 1))
       (node
        (data (color (gaussian 47 25)) (size 0.1))
        (F1 33 220 224 207))))
\end{lstlisting}
The second abstraction corresponds to a ``flower''-like pattern. In this pattern, we could capture even more structure by replacing the variables for the petal colors with a fixed value, since they are similar. Instead of explaining the data as drawn from multiple Gaussians with slightly different means, we could explain the data as generated by a single Gaussian. Our second program transformation, deargumentation, addresses this issue.
\subsection{Deargumentation}
Deargumentation is a program transformation that takes a function $F$ in a program and changes its definition by removing one of the function arguments. Wherever this removed argument is used within $F$, we replace it depending on the values that the overall program assigns to this argument in all calls to $F$. Depending on how we map these argument values to replacements within $F$ (specified via \texttt{replacement-function}), we create different program transformations.

\begin{lstlisting}[frame=trbl]
(define (deargument replacement-function program abstraction variable)
  (let* ([abstraction* (remove-abstraction-variable replacement-function program abstraction variable)])
    (if (null? abstraction*)
        '()
        (let* ([program+abstraction* (update-abstraction program abstraction*)]
               [program* (remove-application-argument program+abstraction* abstraction variable)])
          program*))))
\end{lstlisting}
The abstraction whose variable is being removed keeps the same body, but the variable removed is now assigned a value within the body instead of having its value passed in as an argument.
\begin{lstlisting}[frame=trbl]
(define (remove-abstraction-variable replacement-function program abstraction variable)
  (let* ([variable-instances (find-variable-instances program abstraction variable)]
         [variable-definition (replacement-function program abstraction variable variable-instances)])
    (if (equal? variable-definition NO-REPLACEMENT)
        '()
        (let* ([new-body `((lambda (,variable) ,(abstraction->body abstraction)) ,variable-definition)]
               [new-variables (delete variable (abstraction->vars abstraction))])
          (make-named-abstraction (abstraction->name abstraction) new-body new-variables)))))

(define (program->abstraction-applications program target-abstraction)
  (define (target-abstraction-application? sexpr)
    (if (non-empty-list? sexpr)
        (if (equal? (first sexpr) (abstraction->name target-abstraction))
            #t
            #f)
        #f))
  (let* ([abstraction-bodies (map abstraction->body (program->abstractions program))]
         [possible-locations (pair (program->body program) abstraction-bodies)])
    (deep-find-all target-abstraction-application? possible-locations)))

(define (deep-find-all pred? sexp)
  (filter pred? (all-subexprs sexp)))

(define (all-subexprs t)
  (let loop ([t (list t)])
    (cond [(null? t) '()]
          [(primitive? (first t)) (loop (rest t))]
          [else (pair (first t) (loop (append (first t) (rest t))))])))

(define (find-variable-instances program abstraction variable)
  (let* ([abstraction-applications (program->abstraction-applications program abstraction)]
         [variable-position (abstraction->variable-position abstraction variable)]
         [variable-instances (map (curry ith-argument variable-position) abstraction-applications)])
    variable-instances))

(define (ith-argument i function-application)
  (list-ref function-application (+ i 1)))
\end{lstlisting}
After the abstraction for function $F$ has been adjusted, we change all applications of $F$ by removing the value corresponding to the removed argument.
\begin{lstlisting}[frame=trbl]
(define (remove-application-argument program abstraction variable)
  (define (abstraction-application? sexpr)
    (if (non-empty-list? sexpr)
        (equal? (first sexpr) (abstraction->name abstraction))
        #f))
  (define (change-application variable-position application)
    ;; in case one of the arguments is an application of the
    ;; abstraction currently being deargumented:
    (define (change-recursive-arguments argument)
      (if (abstraction-application? argument)
          (change-application variable-position argument)
          argument))
    (let* ([ith-removed (remove-ith-argument variable-position application)])
      (map change-recursive-arguments ith-removed)))
  (let* ([variable-position (abstraction->variable-position abstraction variable)]
         [program-sexpr (program->sexpr program)]
         [changed-sexpr (sexp-search abstraction-application? (curry change-application variable-position) program-sexpr)]
         [program* (sexpr->program changed-sexpr)])
    program*))

(define (remove-ith-argument i function-application)
  (append (take function-application (+ i 1)) (drop function-application (+ i 2))))

(define (sexp-search pred? func sexp)
  (if (pred? sexp)
      (func sexp)
      (if (list? sexp)
          (map (curry sexp-search pred? func) sexp)
          sexp)))
\end{lstlisting}
In the following, we demonstrate that this transformation is useful for compactly representing continuous values that may have been distorted by noise and for inducing recursive structure.

\subsubsection{Compactly representing noisy data}

When we unify two expressions to create abstractions, all places where the expressions do not exactly match result in the creation of a variable. For example, given \texttt{(+ 2 1.99)} and \texttt{(+ 2 2.01)}, we unify to \texttt{(+ 2 V)}. If we know that the system we are modeling is noisy, we may want to treat these two expressions as essentially identical, i.e., unifying to \texttt{(+ 2 2)} instead. We achieve this effect by using the deargumentation transform with a \texttt{replacment-function} called \texttt{noisy-number-replacment}. This function replaces a variable within an abstraction with the mean of the values of all its instances.

\begin{lstlisting}[frame=trbl]
(define (noisy-number-replacement program abstraction variable variable-instances)
  (if (all (map number? variable-instances))
      (my-mean variable-instances)
      NO-REPLACEMENT))
\end{lstlisting}

Suppose that abstraction results in this program:
\begin{lstlisting}[mathescape=true]
(begin
  (define flower
    (lambda (V1 V2 V3 V4)
      (node (data (color (gaussian V1 25)) (size 0.3))
            (node (data (color (gaussian V2 25)) (size 0.3)))
            (node (data (color (gaussian V3 25)) (size 0.3)))
            (node (data (color (gaussian V4 25)) (size 0.3))))))
  (uniform-choice
   (flower 200 213 207 211)
   (flower 33 220 224 207)))
$\includegraphics[scale=.6]{../figures/noisyNumberProgram.pdf}$
\end{lstlisting}
If we apply the deargumentation transform to the \texttt{flower} function and variabe \texttt{V2}, we identify the two instances of \texttt{V2}, $213$ and $220$, and we are going to replace this variable by its mean, $216.5$.  The abstraction \texttt{flower} is then changed using \texttt{remove-abstraction-variable} to:
\begin{lstlisting}
(define flower
  (lambda (V1 V3 V4)
    ((lambda (V2)
       (node (data (color (gaussian V1 25)) (size 0.3))
             (node (data (color (gaussian V2 25)) (size 0.3)))
             (node (data (color (gaussian V3 25)) (size 0.3)))
             (node (data (color (gaussian V4 25)) (size 0.3)))))
     216.5)))
\end{lstlisting}
The program is now adjusted to incorporate the new version of \texttt{flower} using \texttt{remove-application-argument}:
\begin{lstlisting}[mathescape=true]
(begin
  (define flower
    (lambda (V1 V3 V4)
      ((lambda (V2)
         (node (data (color (gaussian V1 25)) (size 0.3))
               (node (data (color (gaussian V2 25)) (size 0.3)))
               (node (data (color (gaussian V3 25)) (size 0.3)))
               (node (data (color (gaussian V4 25)) (size 0.3)))))
       216.5)))
  (uniform-choice
   (flower 200 207 211)
   (flower 33 224 207)))
$\includegraphics[scale=.6]{../figures/noisyNumberTrans1.pdf}$
\end{lstlisting}
The more applications of \texttt{flower} exist within the program, the bigger the impact of this simplification. This transform will affect the likelihood of the data under the program. Since we search based on posterior probability, incorporating both prior and likelihood, we trade off improvements in model complexity and fit to the data in a principled way. If we had applied the deargumentation transform to the \texttt{flower} argument \texttt{V1}, which is used with values that are less similar, we would get a program that gives lower likelihood to the data than the program generated by removing \texttt{V2}:
\begin{lstlisting}[mathescape=true]
(begin
  (define flower
    (lambda (V2 V3 V4)
      ((lambda (V1)
         (node (data (color (gaussian V1 25)) (size 0.3))
               (node (data (color (gaussian V2 25)) (size 0.3)))
               (node (data (color (gaussian V3 25)) (size 0.3)))
               (node (data (color (gaussian V4 25)) (size 0.3)))))
       116.5)))
  (uniform-choice
   (flower 213 207 211)
   (flower 220 224 207)))
$\includegraphics[scale=.6]{../figures/noisyNumberTrans2.pdf}$
\end{lstlisting}
\subsubsection{Merging identical variables}
Abstraction creates functions whose arguments represent expression parts that differ between specific abstraction instances. When two different arguments take on the same set of values, we can compress the program by merging these arguments. We achieve this using the deargumentation transform with the following \texttt{replacement-function}:
\begin{lstlisting}[frame=trbl]
(define (same-variable-replacement program abstraction variable variable-instances)
  (let* ([possible-match-variables (delete variable (abstraction->vars abstraction))])
    (find-matching-variable program abstraction variable-instances possible-match-variables)))

(define (find-matching-variable program abstraction variable-instances possible-match-variables)
  (define (my-equal? a b) 
    (if (and (pair? a) (pair? b)) 
        (and (my-equal? (car a) (car b)) (my-equal? (cdr a) (cdr b))) 
        (if (and (number? a) (number? b))
            #t
            (eq? a b)))) 
  (if (null? possible-match-variables)
      NO-REPLACEMENT
      (let* ([hypothesis-variable (uniform-draw possible-match-variables)]
             [hypothesis-instances (find-variable-instances program abstraction hypothesis-variable)])
        (if (my-equal? hypothesis-instances variable-instances)
            hypothesis-variable
            (find-matching-variable program abstraction variable-instances (delete hypothesis-variable possible-match-variables))))))
\end{lstlisting}
%% FIXME: Change code to do set comparison as described in the text

If two variables take on the same value in all of their applications, we consider them identical. In order to allow for more compressive proposals, we also consider any two numbers to be equal. To detect matching variables, we first choose an abstraction, choose a variable for this abstraction, and then check whether any of the other variables of this abstraction have a matching set of instance values. If a variable matches, we return it as the new definition of the variable that is being removed\footnote{Alternatively, we could directly replace the variable name in the abstraction body and get slightly better compression. We chose to go the redefinition route in order to keep the interface within the class of deargumentation transforms.}.
  
Suppose again that we start with the following program:
\begin{lstlisting}[mathescape=true]
(begin
  (define flower
    (lambda (V1 V2 V3 V4)
      (node (data (color (gaussian V1 25)) (size 0.3))
            (node (data (color (gaussian V2 25)) (size 0.3)))
            (node (data (color (gaussian V3 25)) (size 0.3)))
            (node (data (color (gaussian V4 25)) (size 0.3))))))
  (uniform-choice
   (flower 200 213 207 211)
   (flower 33 220 224 207)))
$\includegraphics[scale=.6]{../figures/noisyNumberProgram.pdf}$   
\end{lstlisting}

If we apply the deargumentation transform with \texttt{same-variable-replacement} to variable \texttt{V2}, we get a program that generates data similar to the original as long as \texttt{V2} is matched to a variable that is a ``petal'', i.e., \texttt{V3} (shown here) or \texttt{V4}:

\begin{lstlisting}[mathescape=true]
(begin
  (define flower
    (lambda (V1 V3 V4)
      ((lambda (V2)
         (node (data (color (gaussian V1 25)) (size 0.3))
               (node (data (color (gaussian V2 25)) (size 0.3)))
               (node (data (color (gaussian V3 25)) (size 0.3)))
               (node (data (color (gaussian V4 25)) (size 0.3)))))
       V3)))
  (uniform-choice
   (flower 200 207 211)
   (flower 33 224 207)))
$\includegraphics[scale=.6]{../figures/sameVarTrans1.pdf}$
\end{lstlisting}

If the variable does not match a petal---which is only the case for \texttt{V1}---we get a program that generates data that is unlike the data generated by the original program:

\begin{lstlisting}[mathescape=true]
(begin
  (define flower
    (lambda (V2 V3 V4)
      ((lambda (V1)
         (node (data (color (gaussian V1 25)) (size 0.3))
               (node (data (color (gaussian V2 25)) (size 0.3)))
               (node (data (color (gaussian V3 25)) (size 0.3)))
               (node (data (color (gaussian V4 25)) (size 0.3)))))
       V2)))
  (uniform-choice
   (flower 213 207 211)
   (flower 220 224 207)))
$\includegraphics[scale=.6]{../figures/sameVarTrans2.pdf}$
\end{lstlisting}



\subsubsection{Inducing recursive functions}
We illustrate how recursive patterns can be discovered using Deargumentation and a \texttt{replacement-function} called \texttt{recursion-replacement} that redefines a variable as a choice between a recursive call and a non-recursive call.  The basic idea is to check whether variable instances are calls to the function being deargumented.  If they are then there is a recursion.  The probability of recursing is a function of how many variable instances are recursive function calls.  We also check to see whether this transformation creates an infinite loop.  This approach is admittedly very limited in what it can capture, but we use it as a starting point to illustrate how recursion might be identified.
\begin{lstlisting}[frame=trbl]
(define (recursion-replacement program abstraction variable variable-instances)
  (let* ([valid-variable-instances (remove has-variable? variable-instances)]
         [recursive-calls (filter (curry abstraction-application? abstraction) valid-variable-instances)]
         [non-recursive-calls (remove (curry abstraction-application? abstraction) valid-variable-instances)]
         [terminates (terminates? program (abstraction->name abstraction) non-recursive-calls)]) 
    (if (or (null? valid-variable-instances) (null? recursive-calls) (not terminates))
        NO-REPLACEMENT
        (let* ([prob-of-recursion (/ (length recursive-calls) (length valid-variable-instances))])
          `(if (flip ,prob-of-recursion) ,(first recursive-calls) (uniform-choice  ,@non-recursive-calls))))))

(define (terminates? program init-abstraction-name non-recursive-calls)
  (define abstraction-statuses (make-hash-table eq?))

  (define (initialize-statuses)
    (let ([abstraction-names (map abstraction->name (program->abstractions program))])
      (begin
        (map (curry hash-table-set! abstraction-statuses) abstraction-names (make-list (length abstraction-names) 'unchecked))
        (hash-table-set! abstraction-statuses init-abstraction-name 'checking))))

  (define (status? name)
    (hash-table-ref abstraction-statuses name))

  (define (set-status! name new-status)
    (hash-table-set! abstraction-statuses name new-status))
  
  (define (terminating-abstraction? abstraction-name)
    (cond [(eq? (status? abstraction-name) 'terminates) #t]
          [(eq? (status? abstraction-name) 'checking) #f]
          [(eq? (status? abstraction-name) 'unchecked)
           (begin
             (set-status! abstraction-name 'checking)
             (if (base-case? (program->abstraction-body program abstraction-name))
                 (begin
                   (set-status! abstraction-name 'terminates)
                   #t)
                 #f))]))
  
  (define (base-case? sexpr)
    (cond [(branching? sexpr) (list-or (map base-case?  (get-branches sexpr)))]
          [(application? sexpr) (if (any-abstraction-application? sexpr)
                                    (and (terminating-abstraction? (operator sexpr)) (all (map base-case? (operands sexpr))))
                                    (all (map base-case? (operands sexpr))))]
          [else #t]))
  (begin
    (initialize-statuses)
    (list-or (map base-case? non-recursive-calls))))
\end{lstlisting}

The program we transform is \texttt{(begin (node (node a)))} and we can apply the abstraction transformation to get the following:
\begin{lstlisting}[mathescape=true]
(begin
  (define F1 
    (lambda (x)
      (node x))
    (F1 (F1 a))))
$\includegraphics[scale=.25]{../figures/recBef.pdf}$
\end{lstlisting}

Now we apply Deargumentation and remove F1's argument.  The first step is to change the definition of F1 (via \texttt{remove-abstraction-variable}) so that x is drawn from a distribution of past instances of the argument like so:
\begin{lstlisting}
(begin
  (define (F1 x)
    ((lambda (x) (node x)) (if (flip .5) (F1 a) a)))
  (F1 (F1 a)))
\end{lstlisting}
Here the instances of x (i.e., what was passed into the function F1) are \texttt{'(F1 a)}.   The final step is to remove the argument from F1 and any applications of F1 resulting in the program using \texttt{remove-application-argument}.
\begin{lstlisting}[mathescape=true]
(begin
  (define (F1)
    ((lambda (x) (node x)) (if (flip .5) (F1) a)))
  (F1))
$\includegraphics[scale=.25]{../figures/recAft.pdf}$
\end{lstlisting}


\subsubsection{Inducing noisy data constructors}
We generated colors for our trees using Gaussians and one motivation for this would be to model the random processes in the environment.  There is another, more subtle, reason to introduce a noise process into our programs related to representation.  Randomness in data constructors (such as \texttt{color}) can potentially allow for compact representations of patterns in the presence of noise.  To get a sense of this idea look at figure \ref{fig:noiseCons}. 
\begin{figure}[h]
\begin{center}
\includegraphics[scale=.60]{../figures/noisyConstructor.pdf}
\end{center}
\caption{Some trees.}
\label{fig:noiseCons}
\end{figure}
Without a noisy data constructor (i.e., if there were no Gaussian inside calls to \texttt{color}) we might create the following generative model:
\begin{lstlisting}
(if (flip)
 (node (data (color 20) (size .5)) (node (data (color 255) (size .5))))
 (node (data (color 20) (size .5)) (node (data (color 105) (size .5)))))
\end{lstlisting}
This seems like a big penalty as far as model representation size goes since the probability of drawing the darker colored object may be incredibly small.  By having a noisy constructor we can model the data more compactly as simply 
\begin{lstlisting}
(node (data (color 20) (size .5)) (node (data (color (gaussian 255 25) (size .5)))))
\end{lstlisting}

The use of the noisy color constructor in our programs was built into the \texttt{incorporate-data} function as noted earlier.  Here we give an example of how the compactness of noisy constructors might be used to ``learn'' them from data.

We make the following minor adjustments to \texttt{incorporate-data} and \texttt{noisy-number-replacement}. The first is to not have \texttt{incorporate-data} automatically add a call to \texttt{gaussian}.
\begin{lstlisting}[frame=trblsingle]
(define (node-data->expression lst)
  `(data (color (gaussian ,(first (second lst)) 25))
	 (size ,(first (third lst)))))
\end{lstlisting}
becomes
\begin{lstlisting}[frame=trblsingle]
(define (node-data->expression lst)
  `(data (color ,(first (second lst))) (size ,(first (third lst)))))
\end{lstlisting}
Instead of having \texttt{noisy-number-replacement} return the sample mean in a deargument move we have it return a call to the \texttt{gaussian} function and use the sample mean and variance as parameters.
\begin{lstlisting}[frame=trbl]
(define (noisy-number-replacement program abstraction variable variable-instances)
  (if (all (map number? variable-instances))
      (my-mean variable-instances)
      NO-REPLACEMENT))
\end{lstlisting}
becomes
\begin{lstlisting}[frame=trblsingle]
(define (noisy-number-replacement program abstraction variable variable-instances)
  (if (all (map number? variable-instances))
      (let* ([instances-mean (my-mean variable-instances)]
	     [instances-deviation (sqrt (sample-variance variable-instances))])
	`(gaussian ,instances-mean ,instances-deviation))
      NO-REPLACEMENT))
\end{lstlisting}

We use the following program to generate three node structures where there is some variance in the color of the third node.
\begin{lstlisting}[mathescape=true]
(define (three-node shade)
(node (data (color 0) (size .3))
      (node (data (color 0) (size .3)) 
	    (node (data (color (gaussian shade 10)) (size .3))))))

(list (three-node 200) (three-node 200) (three-node 200) (three-node 200) (three-node 200) (three-node 200) (three-node 200) (three-node 200) (three-node 200) (three-node 200))
$\includegraphics[scale=.6]{../figures/learnnoise-data.pdf}$
\end{lstlisting}
Data incorporation gives us a program that uniformly chooses between ten generated three node structures.
\begin{lstlisting}[mathescape=true]
(lambda ()
  (uniform-choice
   (node (data (color 0) (size 0.3))
	 (node (data (color 0) (size 0.3))
	       (node (data (color 209.0) (size 0.3)))))
   (node (data (color 0) (size 0.3))
	 (node (data (color 0) (size 0.3))
	       (node (data (color 196.0) (size 0.3)))))
                $\vdots$
   (node (data (color 0) (size 0.3))
	 (node (data (color 0) (size 0.3))
	       (node (data (color 206.0) (size 0.3)))))))
\end{lstlisting}
Bayesian program merging with the above modifications results in a compressed program that creates a function for the three node structure that adds noise to the color of the third node.  We ran the system with $\alpha=3$, beam width 1, and depth 10 for this example.
\begin{lstlisting}[mathescape=true]
(begin
    (define F2 (lambda (V2) (data (color V2) (size 0.3))))
    (define F1
      (lambda ()
        ((lambda (V1)
           (node (F2 0) (node (F2 0) (node (F2 V1)))))
          (gaussian 202.3 9.286190463980603))))
    (lambda ()
      (uniform-choice (F1) (F1) (F1) (F1) (F1) (F1) (F1)
        (F1) (F1) (F1))))
$\includegraphics[scale=.6]{../figures/learnnoise-output.pdf}$
\end{lstlisting}

The discussion above hints at the benefits of using probabilistic data constructors and probabilistic programs in general with respect to representation, but understanding the full implications of such a design decision and its impact on program induction are left as future work.


\subsubsection{Deargumentation as program induction}

In some sense the problem we are trying to address with the Deargumentation is program induction itself.  We can view the values for each variable as data generated by some process we would like to identify, i.e., there is some sort of common computation between variable values that we would like to represent as a program.  In the case of the noisy-number transformation we can view the generation of values for a single variable as coming from the same process, here we restrict the program representing this process to a Gaussian.  Similarly, in the case of the recursive transformation we can view the generation of values for a single variable as coming from a the same process, but we restrict the program representing this process to be the function being deargumented.  In the case of the same variable transformation we can view the generation of values for multiple variables as coming from the same process and we implicitly use the program for one variable as the generating process for the other.  

Hence, it is natural to ask whether we could reformulate the Deargumentation transform as recursive calls to the Bayesian program merging procedure in an attempt to find programs for generating the values of the variables.  One can think of this as recursively squeezing out the randomness in the data where the invented abstractions create a separation between the ``structured'' parts of the data (the fixed common sub-expressions) and the random parts of the data (the variables in the expression patterns).  As an abstraction is applied one gets more data for the random parts and could conceivably attempt to learn a program to generate this data, i.e., separate out even more structure.


\newpage
\section{Search strategy: Beam search}

We are interested in programs with high posterior probability. Many different search strategies are possible, but in this report, we limit ourselves to beam search using the posterior probability as a search heuristic.

\begin{lstlisting}[frame=trbl]
(define (beam-search data init-program beam-size depth)
  (let* ([top-transformations 
          (sort-by-posterior
           data 
           (beam-learn-search-transformations data init-program beam-size depth))])
    (if (null? top-transformations)
        init-program
        (program+->program (first top-transformations)))))

(define (beam-search-transformations data program beam-size depth)
  (let ([init-program+ (make-program+ program 0 (log-likelihood data program 10) (log-prior program) #f)])
    (depth-iterated-transformations (lambda (programs+) (best-n data programs+ beam-size)) init-program+ depth)))

(define (best-n data programs+ n)
  (max-take (sort-by-posterior data programs+) n))
\end{lstlisting}
The main part of the search is performed by \texttt{depth-iterated-transformations}, which recursively applies program transformations to the best programs at a given search depth and then filters the results to get the best programs for the next depth.
\begin{lstlisting}[frame=trbl]
(define (depth-iterated-transformations cfilter program+ depth)
  (let* ([transformed-programs+ (apply-and-filter-transformations depth cfilter program+)])
    (delete '()  (append transformed-programs+
                         (apply append (map (lambda (prog) 
                                              (depth-iterated-transformations cfilter prog (- depth 1))) 
                                            transformed-programs+))))))
\end{lstlisting}
We reduce the amount of computation required when choosing the best programs at each level of search by separating program transformations that preserve semantics from those that do not.  Marking programs based on their transformation type allows us to reuse likelihood for a program that was created by a semantics preserving transformation.
\begin{lstlisting}[frame=trbl]
(define (apply-and-filter-transformations depth cfilter program+)
  (if (= depth 0)
      '()
      (let* ([semantics-preserved-programs+ (apply-transformations program+ semantic-preserving-transformations #t)]
             [semantics-changed-programs+ (apply-transformations program+ semantic-changing-transformations #f)])
        (cfilter (append semantics-preserved-programs+ semantics-changed-programs+)))))

(define (apply-transformations program+ transformations semantics-preserving)
  (let* ([program (program+->program program+)]
         [transformed-programs (delete '() (concatenate (map (lambda (transform) (transform program #t)) transformations)))]
         [transformed-programs+ (map (lambda (program) (program+->program-transform semantics-preserving program+ program)) transformed-programs)])
    transformed-programs+))
\end{lstlisting}
The posterior distribution is estimated by combining an estimate of the likelihood with the computation of the prior.  
\begin{lstlisting}[frame=trbl]
(define (sort-by-posterior data programs+)
  (let* ([programs (map program+->program programs+)]
         [semantics-flags (map program+->semantics-preserved programs+)]
         [log-priors (map log-prior programs)]
         [log-likelihoods (map (lambda (prog+ semantics-flag)
                                 (if semantics-flag
                                     (program+->log-likelihood prog+)
                                     (log-likelihood data (program+->program prog+) 10))) programs+ semantics-flags)]
         [posteriors (map + log-priors log-likelihoods)] 
         [new-programs+ (map make-program+ programs posteriors log-likelihoods log-priors semantics-flags)]
         [posteriors> (lambda (a b) (> (program+->posterior a) (program+->posterior b)))])
    (my-list-sort posteriors> new-programs+)))
\end{lstlisting}


\newpage
\section{Examples}
\subsection{Single color flower}
We use the program below to generate ten instances of flower that have the same color.  
\begin{lstlisting}[mathescape=true]
(define (flower shade)
  (node (data (color (gaussian shade 25)) (size .3))
        (petal shade)
        (petal shade)
        (petal shade)))

(define (petal shade)
  (node (data (color (gaussian shade 25)) (size .3))))

(repeat 10 (lambda () (flower 20)))
$\includegraphics[scale=.35]{../figures/single-color-data.pdf}$
\end{lstlisting}
An expression for these ten flowers was created using data incorporation and this expression was compressed into the program below.  The function \texttt{F1} is a function that takes no arguments and creates a flower with petals that are all the same color.  We ran the system with $\alpha=1$, beam width 1, and depth 10 for this example.
\begin{lstlisting}[mathescape=true]
(begin
  (define F2
    (lambda (V5)
      (data (color (gaussian V5 25)) (size 0.3))))
  (define F1
    (lambda ()
      ((lambda (V4)
	 ((lambda (V2)
	    ((lambda (V1)
	       ((lambda (V3)
		  (node (F2 V1) (node (F2 V2))
			(node (F2 V3)) (node (F2 V4))))
		17.2))
	     32.9))
	  2.0))
       19.7)))
  (lambda ()
    (uniform-choice (F1) (F1) (F1) (F1) (F1) (F1) (F1)
		    (F1) (F1) (F1))))

$\includegraphics[scale=.35]{../figures/single-color-model.pdf}$
\end{lstlisting}
\subsection{Multiple color flower}
Here we use a similar program to generate ten instances of flower that have alternating colors.  
\begin{lstlisting}[mathescape=true]
(define (flower shade)
  (node (data (color (gaussian 0 25)) (size .3))
        (petal shade)
        (petal shade)
        (petal shade)))

(define (petal shade)
  (node (data (color (gaussian shade 25)) (size .3))))

(repeat 10 (lambda () (flower (if (flip) 100 220))))
$\includegraphics[scale=.35]{../figures/paramPartsData.pdf}$
\end{lstlisting}
Bayesian program merging results in the following program.  The abstraction \texttt{F2} corresponds to \texttt{petal} and \texttt{F1} corresponds to \texttt{flower}.  We ran the system with $\alpha=1$ for this example, beam width 1, and depth 10.
\begin{lstlisting}[mathescape=true]
(begin
  (define F2
    (lambda (V5)
      (data (color (gaussian V5 25)) (size 0.3))))
  (define F1
    (lambda (V2)
      ((lambda (V1)
         ((lambda (V3)
            ((lambda (V4)
               (node (F2 V1) (node (F2 V2)) (node (F2 V3))
                     (node (F2 V4))))
             V2))
          V1))
       V2)))
  (lambda ()
    (uniform-choice (F1 91.0) (F1 85.0) (F1 254.0)
                    (F1 234.0) (F1 82.0) (F1 243.0) (F1 104.0))))
$\includegraphics[scale=.35]{../figures/paramPartsModelData.pdf}$
\end{lstlisting}
\subsection{Recursion}
We use the following program to generate a tree made of a single line of nodes.  We ran the system with $\alpha=1$ for this example, beam width 1, and depth 10.
\begin{lstlisting}[mathescape=true]
(define (line)
  (if (flip .2)
      (node (data (color 200) (size .5)))
      (node (data (color 200) (size .5)) (line))))
$\includegraphics[scale=.35]{../figures/recursionData.pdf}$
\end{lstlisting}
Bayesian program merging finds the following recursion.
\begin{lstlisting}[mathescape=true]
(begin 
  (define F3 (lambda () (lambda () (F1))))
  (define F2
    (lambda () (data (color (gaussian 200 25)) (size 0.5))))
  (define F1
    (lambda ()
      ((lambda (V1) (node (F2) V1))
       (if (flip 8/9) (F1) (node (F2))))))
  (lambda ()
    (uniform-choice
     (list (F3) (F3) (F3) (F3) (lambda () (node (F2))))))))
$\includegraphics[scale=.25]{../figures/recursionModel.pdf}$
\end{lstlisting}
The recursions have to be of a certain length before the tradeoff between prior and likelihood favors the recursive programs, which are usually smaller.  We can adjust where this tradeoff happens using the size constant, $\alpha$, in the prior. 
\subsection{Vine}
Here we use Bayesian program merging to model a vine with flowers from a single instance.  This example demonstrates multiple types of program transformations used on the same data.  We ran the system with $\alpha=1$ for this example, beam width 1, and depth 10.
\begin{lstlisting}[mathescape=true]
(define (vine)
  (if (flip .1)
      (node (data (color 100) (size .1)))
      (node (data (color 100) (size .1)) (vine) (flower))))

(define (flower)
  (node (data (color (gaussian 20 25)) (size .3))
        (petal 20)
        (petal 20)))

(define (petal shade)
  (node (data (color (gaussian shade 25)) (size .3))))
$\includegraphics[scale=.35]{../figures/vineData.pdf}$
\end{lstlisting}
Here we see \texttt{F1} corresponds roughly to \texttt{vine} and is passed a single color for the flower parts and the size has been fixed within \texttt{F1}, \texttt{F2} corresponds to the data part of the flower.
\begin{lstlisting}[mathescape=true]
(begin
  (define F3 (lambda () (F2 100 0.1)))
  (define F2
    (lambda (V4 V5)
      (data (color (gaussian V4 25)) (size V5))))
  (define F1
    (lambda (V2)
      ((lambda (V3)
         ((lambda (V1)
            (node (F3) V1
                  (node (F2 V2 0.3) (node (F2 V3 0.3)))))
          (if (flip 12/13) (F1 1.0) (node (F3)))))
       7.0)))
  (lambda ()
    (uniform-choice (F1 -14.0))))
$\includegraphics[scale=.25]{../figures/vineModel.pdf}$
\end{lstlisting}
\subsection{Tree}
This example demonstrates learning both a parameterized function and a recursion and applying these functions in multiple places within a program.  The following functions were used to generate some flower patterns and a tree that consists of two branches each of which ends in a different color flower.  We ran the system with $\alpha=3$, beam width 1, and depth 10 for this example.  With $\alpha=1$ we do not get the right tradeoff between prior and likelihood to make introducing recursion worhtwhile although We expect the system could learn a similar program with $\alpha=1$ given training examples with more branch instances, since the amount of program compression would increase.
\begin{lstlisting}[mathescape=true]
(define tree
  (lambda ()
    (uniform-choice
     (node (body) (branch) (branch)))))

(define (body) 
  (data (color (gaussian 50 25)) (size 1)))

(define (branch)
  (if (flip .1)
      (uniform-choice (flower 20) (flower 220))
      (node (branch-info) (branch))))

(define (branch-info)
  (data (color (gaussian 100 25)) (size .1)))

(define (flower shade)
  (node (data (color (gaussian shade 25)) (size .3))
        (petal shade)
        (petal shade)))

(define (petal shade)
  (node (data (color (gaussian shade 25)) (size .3))))
$\includegraphics[scale=.25]{../figures/treeData.pdf}$
\end{lstlisting}
Bayesian model merging produces a program with a similar structure to the original generating program.  \texttt{F3} plays a similar role to the \texttt{flower} function by taking a single color as argument and creating three nodes of size .3 with the passed in color.  \texttt{F2} is a function that creates a branch that ends in a flower with either blue petals or red petals.
\begin{lstlisting}[mathescape=true]
(begin
   (define F4 (lambda (V9 V10) (node (F1 V9 0.1) V10)))
   (define F3
     (lambda (V7)
       ((lambda (V6)
          ((lambda (V8)
             (node (F1 V6 0.3) (node (F1 V7 0.3))
                   (node (F1 V8 0.3))))
           V7))
        V7)))
   (define F2
     (lambda (V3 V4)
       ((lambda (V5) (F4 V3 (F4 V4 V5)))
        (if (flip 9/11)
            (F2 121.0 135.0)
            (uniform-choice (F4 39.0 (F3 47.0)) (F3 187.0))))))
   (define F1
     (lambda (V1 V2)
       (data (color (gaussian V1 25)) (size V2))))
   (lambda ()
     (uniform-choice (F3 2e1) (F3 235.0)
                     (node (F1 34.0 1) (F2 108.0 99.0) (F2 134.0 85.0))
                     (F3 36.0))))
$\includegraphics[scale=.25]{../figures/treeModel.pdf}$
\end{lstlisting}


\newpage
\section{Conclusion}
We presented Bayesian program merging, an approach to inducing generative models from data. The main idea of this approach is to directly translate the data into a program and to then compress this program by identifying repeated computations. We perform this compression of repeated computation using program transformations. We determine the sequence of transformations by using the posterior probability of the program as a search heuristic.

Future improvements of the system described in this paper include more sophisticated search strategies, more efficient ways of computing the likelihood, and the development of a more robust method for identifying recursive patterns.  Another direction is adaptation of this style of probabilistic program induction to the online setting.  

More generally, we can ask whether the idea of regularity in data as repeated computation in the generative process can be used to identify other useful program transformations or, whether it can be applied in a more systematic fashion than motivating a collection of disparate search moves.  

A different and important question is what happens when the data incorporation step is less immediate and training data is not directly computed in terms of some algebraic data type and whether one can impose some semblance of structure on unstructured data in order to use these methods for learning.  

There are many other barriers to overcome before probabilistic program induction can compete with state-of-the-art machine learning algorithms on real world problems, but the increased potential for capturing rich patterns and less dependence on human engineering make research in the subject a worthy pursuit.

\newpage
\bibliographystyle{plain}
\bibliography{bpm-report}
\end{document}
